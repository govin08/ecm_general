{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da95590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from split_data import train_test_split_timeseries, prepare_rl_data, validate_rl_data\n",
    "from offline_RLs import (\n",
    "    BehaviorCloning, ConservativeQL, ImplicitQL, TD3BC, OfflineDDPG,\n",
    "    compare_algorithms, create_replay_buffer_from_data, normalize_data\n",
    ")\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7fe589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Offline RL Algorithms Comparison Test\n",
      "============================================================\n",
      "\n",
      "1. Data Splitting...\n",
      "Data split completed:\n",
      "  Total samples: 120960\n",
      "  Train samples: 96768 (80.0%)\n",
      "  Test samples: 24192 (20.0%)\n",
      "  Train period: 2025-02-17 00:00:00 ~ 2025-02-22 14:23:55\n",
      "  Test period: 2025-02-22 14:24:00 ~ 2025-02-23 23:59:55\n",
      "\n",
      "2. Preparing RL Data...\n",
      "RL data preparation completed:\n",
      "  Samples: 96767\n",
      "  State dim: 11\n",
      "  Action dim: 1\n",
      "  Target variable: ê°€ìŠ¤í„°ë¹ˆ í›„ë‹¨ ì§ˆì†Œì‚°í™”ë¬¼ ë†ë„ (index: 10)\n",
      "  Action bounds: [26.10, 47.19]\n",
      "  Average reward: -5.000\n",
      "  Reward std: 0.207\n",
      "\n",
      "3. Data Validation...\n",
      "âœ“ RL data validation passed successfully\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "path_root = os.path.dirname(os.path.abspath('.'))\n",
    "# data = pd.read_csv(os.path.join(path_root, 'data', 'data0904.csv'))\n",
    "data = pd.read_csv(os.path.join('data0904.csv'))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Offline RL Algorithms Comparison Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë³€ìˆ˜ ì •ì˜\n",
    "action_tag = 'DGAN Compressor ì§ˆì†Œê°€ìŠ¤ ìœ ëŸ‰'\n",
    "target_tag = 'ê°€ìŠ¤í„°ë¹ˆ í›„ë‹¨ ì§ˆì†Œì‚°í™”ë¬¼ ë†ë„'\n",
    "\n",
    "# 1. Train/Test ë¶„í• \n",
    "print(\"\\n1. Data Splitting...\")\n",
    "train_data, test_data, split_info = train_test_split_timeseries(data, test_ratio=0.2)\n",
    "\n",
    "# 2. ê°•í™”í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (Train)\n",
    "print(\"\\n2. Preparing RL Data...\")\n",
    "train_rl_data = prepare_rl_data(train_data, action_tag, target_tag)\n",
    "\n",
    "# 3. ë°ì´í„° ê²€ì¦\n",
    "print(\"\\n3. Data Validation...\")\n",
    "is_valid, validation_info = validate_rl_data(train_rl_data)\n",
    "\n",
    "if not is_valid:\n",
    "    print(\"âŒ Data validation failed!\")\n",
    "    print(validation_info)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7ec76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96767, 11)\n",
      "(96767, 1)\n",
      "(96767,)\n",
      "(96767, 11)\n",
      "(96767,)\n"
     ]
    }
   ],
   "source": [
    "print(train_rl_data['states'].shape)\n",
    "print(train_rl_data['actions'].shape)\n",
    "print(train_rl_data['rewards'].shape)\n",
    "print(train_rl_data['next_states'].shape)\n",
    "print(train_rl_data['done'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6434ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing multiple offline RL algorithms...\n",
      "============================================================\n",
      "Replay buffer created with 96767 samples\n",
      "\n",
      "ğŸš€ Training BC...\n",
      "----------------------------------------\n",
      "Training Behavior Cloning...\n",
      "BC Iteration 200/1000, Loss: 0.176902\n",
      "BC Iteration 400/1000, Loss: 0.239938\n",
      "BC Iteration 600/1000, Loss: 0.173163\n",
      "BC Iteration 800/1000, Loss: 0.165050\n",
      "BC Iteration 1000/1000, Loss: 0.188534\n",
      "âœ… BC training completed!\n"
     ]
    }
   ],
   "source": [
    "name = 'BC'\n",
    "algorithm_class = BehaviorCloning\n",
    "iterations = 5000\n",
    "\n",
    "print(\"Comparing multiple offline RL algorithms...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë°ì´í„° ì •ê·œí™”\n",
    "normalized_data, state_scaler, action_scaler = normalize_data(train_rl_data)\n",
    "\n",
    "# ê³µí†µ íŒŒë¼ë¯¸í„°\n",
    "state_dim = normalized_data['states'].shape[1]\n",
    "action_dim = normalized_data['actions'].shape[1]\n",
    "max_action = 1.0  # ì •ê·œí™” í›„ ëŒ€ëµì ì¸ ìµœëŒ€ê°’\n",
    "\n",
    "# Replay buffer ìƒì„±\n",
    "replay_buffer = create_replay_buffer_from_data(normalized_data)\n",
    "\n",
    "print(f\"\\nğŸš€ Training {name}...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ì•Œê³ ë¦¬ì¦˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "if name == 'BC':\n",
    "    algorithm = algorithm_class(state_dim, action_dim, max_action)\n",
    "    training_results = algorithm.train(replay_buffer, iterations=iterations//5)  # BCëŠ” ë” ì ê²Œ\n",
    "else:\n",
    "    algorithm = algorithm_class(state_dim, action_dim, max_action)\n",
    "    training_results = algorithm.train(replay_buffer, iterations=iterations)\n",
    "\n",
    "\n",
    "results = {\n",
    "    'algorithm': algorithm,\n",
    "    'training_results': training_results,\n",
    "    'state_scaler': state_scaler,\n",
    "    'action_scaler': action_scaler\n",
    "        }\n",
    "\n",
    "print(f\"âœ… {name} training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f37db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = results['algorithm']\n",
    "training_results = results['training_results']\n",
    "state_scaler = results['state_scaler']\n",
    "action_scaler = results['action_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222fe6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_results['losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06895a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Running Algorithm Comparison...\n",
      "Comparing multiple offline RL algorithms...\n",
      "============================================================\n",
      "Replay buffer created with 96767 samples\n",
      "\n",
      "ğŸš€ Training BC...\n",
      "----------------------------------------\n",
      "Training Behavior Cloning...\n",
      "BC Iteration 200/400, Loss: 0.146544\n",
      "BC Iteration 400/400, Loss: 0.175878\n",
      "âœ… BC training completed!\n",
      "\n",
      "ğŸš€ Training CQL...\n",
      "----------------------------------------\n",
      "Training Conservative Q-Learning...\n",
      "CQL Iteration 1000/2000\n",
      "  Critic Loss: 1.844166\n",
      "  CQL Loss: -2.379364\n",
      "  Actor Loss: 14.587351\n",
      "CQL Iteration 2000/2000\n",
      "  Critic Loss: 22.379984\n",
      "  CQL Loss: -6.420288\n",
      "  Actor Loss: 44.663806\n",
      "âœ… CQL training completed!\n",
      "\n",
      "ğŸš€ Training IQL...\n",
      "----------------------------------------\n",
      "Training Implicit Q-Learning...\n",
      "IQL Iteration 1000/2000\n",
      "  Critic Loss: 243.228867\n",
      "  Value Loss: 5.456341\n",
      "  Actor Loss: -6.241105\n",
      "IQL Iteration 2000/2000\n",
      "  Critic Loss: 258.481628\n",
      "  Value Loss: 4.265056\n",
      "  Actor Loss: -4.678623\n",
      "âœ… IQL training completed!\n",
      "\n",
      "ğŸš€ Training TD3+BC...\n",
      "----------------------------------------\n",
      "Training TD3 + BC...\n",
      "TD3+BC Iteration 1000/2000\n",
      "  Critic Loss: 0.147617\n",
      "  Actor Loss: 9.385397\n",
      "TD3+BC Iteration 2000/2000\n",
      "  Critic Loss: 0.131381\n",
      "  Actor Loss: 19.833754\n",
      "âœ… TD3+BC training completed!\n",
      "\n",
      "ğŸš€ Training DDPG...\n",
      "----------------------------------------\n",
      "Training Offline DDPG...\n",
      "Offline DDPG Iteration 1000/2000\n",
      "  Critic Loss: 0.058954\n",
      "  Actor Loss: 8.766506\n",
      "Offline DDPG Iteration 2000/2000\n",
      "  Critic Loss: 0.065653\n",
      "  Actor Loss: 19.006336\n",
      "âœ… DDPG training completed!\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ All algorithms training completed!\n",
      "\n",
      "â±ï¸ Total training time: 232.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# 4. ì•Œê³ ë¦¬ì¦˜ ì •ì˜ ë° ë¹„êµ\n",
    "print(\"\\n4. Running Algorithm Comparison...\")\n",
    "\n",
    "algorithms = {\n",
    "    'BC': BehaviorCloning,\n",
    "    'CQL': ConservativeQL, \n",
    "    'IQL': ImplicitQL,\n",
    "    'TD3+BC': TD3BC,\n",
    "    'DDPG': OfflineDDPG\n",
    "}\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "start_time = time.time()\n",
    "results = compare_algorithms(algorithms, train_rl_data, iterations=2000)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nâ±ï¸ Total training time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c524fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "print(\"\\n5. Results Analysis...\")\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Offline RL Algorithms Training Progress', fontsize=16)\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    training_results = result['training_results']\n",
    "    \n",
    "    if 'losses' in training_results:  # BCì˜ ê²½ìš°\n",
    "        axes[row, col].plot(training_results['losses'], label='Loss')\n",
    "        axes[row, col].set_title(f'{name} - Loss')\n",
    "    elif 'actor_losses' in training_results:  # ë‚˜ë¨¸ì§€ ì•Œê³ ë¦¬ì¦˜ë“¤\n",
    "        if training_results['actor_losses']:\n",
    "            axes[row, col].plot(training_results['actor_losses'], label='Actor Loss', alpha=0.7)\n",
    "        if 'critic_losses' in training_results:\n",
    "            # critic lossëŠ” ë” ìì£¼ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ ê°„ê²©ì„ ë§ì¶¤\n",
    "            critic_x = np.arange(len(training_results['critic_losses']))\n",
    "            axes[row, col].plot(critic_x, training_results['critic_losses'], \n",
    "                              label='Critic Loss', alpha=0.7)\n",
    "        axes[row, col].set_title(f'{name} - Training Losses')\n",
    "    \n",
    "    axes[row, col].set_xlabel('Iterations')\n",
    "    axes[row, col].set_ylabel('Loss')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# ë¹ˆ subplot ì œê±°\n",
    "if len(results) < 6:\n",
    "    axes[1, 2].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ëª‡ ê°œ ìƒ˜í”Œë¡œ)\n",
    "print(\"\\n6. Simple Performance Test...\")\n",
    "\n",
    "# Test ë°ì´í„°ë„ RL í˜•íƒœë¡œ ë³€í™˜\n",
    "test_rl_data = prepare_rl_data(test_data, action_tag, target_tag)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ì •ê·œí™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "test_normalized_data, _, _ = normalize_data(test_rl_data)\n",
    "\n",
    "# ê° ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "performance_results = {}\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    algorithm = result['algorithm']\n",
    "    state_scaler = result['state_scaler']\n",
    "    action_scaler = result['action_scaler']\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ëª‡ ê°œë¡œ ì„±ëŠ¥ í™•ì¸\n",
    "    n_test_samples = min(100, len(test_rl_data['states']))\n",
    "    test_states = test_rl_data['states'][:n_test_samples]\n",
    "    test_actions = test_rl_data['actions'][:n_test_samples]\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    test_states_norm = state_scaler.transform(test_states)\n",
    "    test_actions_norm = action_scaler.transform(test_actions)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì•¡ì…˜\n",
    "    predicted_actions_norm = []\n",
    "    for state in test_states_norm:\n",
    "        pred_action = algorithm.select_action(state)\n",
    "        predicted_actions_norm.append(pred_action)\n",
    "    \n",
    "    predicted_actions_norm = np.array(predicted_actions_norm)\n",
    "    \n",
    "    # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜\n",
    "    predicted_actions = action_scaler.inverse_transform(predicted_actions_norm)\n",
    "    \n",
    "    # MSE ê³„ì‚°\n",
    "    mse = np.mean((predicted_actions.flatten() - test_actions[:n_test_samples].flatten()) ** 2)\n",
    "    mae = np.mean(np.abs(predicted_actions.flatten() - test_actions[:n_test_samples].flatten()))\n",
    "    \n",
    "    performance_results[name] = {\n",
    "        'mse': mse,\n",
    "        'mae': mae\n",
    "    }\n",
    "    \n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"{'Algorithm':<10} {'MSE':<10} {'MAE':<10} {'Status':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_mse_algo = min(performance_results.items(), key=lambda x: x[1]['mse'])\n",
    "best_mae_algo = min(performance_results.items(), key=lambda x: x[1]['mae'])\n",
    "\n",
    "for name, perf in performance_results.items():\n",
    "    status = \"\"\n",
    "    if name == best_mse_algo[0]:\n",
    "        status += \"ğŸ†MSE \"\n",
    "    if name == best_mae_algo[0]:\n",
    "        status += \"ğŸ†MAE\"\n",
    "    \n",
    "    print(f\"{name:<10} {perf['mse']:<10.4f} {perf['mae']:<10.4f} {status:<15}\")\n",
    "\n",
    "print(f\"\\nBest MSE: {best_mse_algo[0]} ({best_mse_algo[1]['mse']:.4f})\")\n",
    "print(f\"Best MAE: {best_mae_algo[0]} ({best_mae_algo[1]['mae']:.4f})\")\n",
    "\n",
    "# 8. ì•¡ì…˜ ì˜ˆì¸¡ ë¹„êµ ì‹œê°í™”\n",
    "print(\"\\n8. Action Prediction Visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Action Predictions vs Ground Truth (First 50 samples)', fontsize=16)\n",
    "\n",
    "n_viz_samples = min(50, len(test_rl_data['states']))\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    algorithm = result['algorithm']\n",
    "    state_scaler = result['state_scaler'] \n",
    "    action_scaler = result['action_scaler']\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    test_states_norm = state_scaler.transform(test_rl_data['states'][:n_viz_samples])\n",
    "    predicted_actions_norm = []\n",
    "    \n",
    "    for state in test_states_norm:\n",
    "        pred_action = algorithm.select_action(state)\n",
    "        predicted_actions_norm.append(pred_action)\n",
    "    \n",
    "    predicted_actions = action_scaler.inverse_transform(np.array(predicted_actions_norm))\n",
    "    actual_actions = test_rl_data['actions'][:n_viz_samples]\n",
    "    \n",
    "    # í”Œë¡¯\n",
    "    x_axis = np.arange(n_viz_samples)\n",
    "    axes[row, col].plot(x_axis, actual_actions.flatten(), 'b-', label='Actual', linewidth=2)\n",
    "    axes[row, col].plot(x_axis, predicted_actions.flatten(), 'r--', label='Predicted', linewidth=2, alpha=0.7)\n",
    "    axes[row, col].set_title(f'{name}')\n",
    "    axes[row, col].set_xlabel('Sample')\n",
    "    axes[row, col].set_ylabel('Action Value')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# ë¹ˆ subplot ì œê±°\n",
    "if len(results) < 6:\n",
    "    axes[1, 2].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ‰ All tests completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_wp (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
